<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/92b53c90e215dddf.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-dd4c7845ef786e22.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-44d6b2258059bd78.js" async=""></script><script src="/_next/static/chunks/252-e3d66a9ba9501477.js" async=""></script><script src="/_next/static/chunks/748-b9229f4347fd4d10.js" async=""></script><script src="/_next/static/chunks/app/page-9581e42e39deaab7.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Selena Song</title><meta name="description" content="Research Assistant at UC San Diego."/><meta name="author" content="Selena Song"/><meta name="keywords" content="Selena Song,PhD,Research,"/><meta name="creator" content="Selena Song"/><meta name="publisher" content="Selena Song"/><meta property="og:title" content="Selena Song"/><meta property="og:description" content="Research Assistant at UC San Diego."/><meta property="og:site_name" content="Selena Song&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Selena Song"/><meta name="twitter:description" content="Research Assistant at UC San Diego."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Selena Song</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Selena Song" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Selena Song</h1><p class="text-lg text-accent font-medium mb-1"></p><p class="text-neutral-600 mb-2"></p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><a href="https://scholar.google.com/citations?user=r9zwfxAAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0009-0009-9433-1647" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/Selena-Song" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="/papers/CV_SelenaSong.pdf" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="CV"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-file-text h-5 w-5" aria-hidden="true"><path d="M15 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V7Z"></path><path d="M14 2v4a2 2 0 0 0 2 2h4"></path><path d="M10 9H8"></path><path d="M16 13H8"></path><path d="M16 17H8"></path></svg></a></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a Research Assistant at <strong class="font-semibold text-primary">University of California, San Diego</strong>, working with <a href="https://biweihuang.com/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Biwei Huang</a> and <a href="https://lancelot39.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Dr. Kun Zhou</a>. My research interests lie in <strong class="font-semibold text-primary">Video Generation</strong>, <strong class="font-semibold text-primary">Multimodal Reasoning</strong>, and <strong class="font-semibold text-primary">Causality-Empowered Foundation Models</strong>.</p>
<p class="mb-4 last:mb-0">Previously, I received my Master&#x27;s degree from <strong class="font-semibold text-primary">The University of Tokyo</strong> in 2025, supervised by <a href="https://scholar.google.com/citations?user=Dy8iau4AAAAJ&amp;hl=en" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Yutaka Matsuo</a>. I earned my Bachelor&#x27;s degree in Physics from <strong class="font-semibold text-primary">Fudan University</strong> in 2023.</p>
<p class="mb-4 last:mb-0">ðŸŽ“ <strong class="font-semibold text-primary">I am actively seeking Ph.D. opportunities for Fall 2026!</strong></p></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">Research Interests</h2><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg border border-neutral-200 dark:border-neutral-700" style="opacity:0;transform:translateY(10px)"><h3 class="font-semibold text-accent mb-2">Physics-Aware Video Generation</h3><p class="text-sm text-neutral-600 dark:text-neutral-400 leading-relaxed">Building controllable video generators conditioned on explicit scene states like 3D geometry while enforcing physical constraints and temporal consistency for robust long-horizon video generation.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg border border-neutral-200 dark:border-neutral-700" style="opacity:0;transform:translateY(10px)"><h3 class="font-semibold text-accent mb-2">Multimodal Reasoning</h3><p class="text-sm text-neutral-600 dark:text-neutral-400 leading-relaxed">Developing structured representation learning methods to mitigate shortcut reliance under distribution shift. Learning latent state with explicit constraints from multimodal data to support consistent inference and robust generalization.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg border border-neutral-200 dark:border-neutral-700" style="opacity:0;transform:translateY(10px)"><h3 class="font-semibold text-accent mb-2">Causality-Empowered Foundation Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-400 leading-relaxed">Developing causal abstractions that support interventions and counterfactual reasoning for multimodal perception and decision-making, leveraging causal graphs and invariant mechanisms for out-of-distribution generalization.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight"><a href="https://thrcle421.github.io/DiT-Mem-Web/" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Learning Plug-and-play Memory for Guiding Video Diffusion Models</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Selena Song</span><sup class="ml-0 text-accent">â€ </sup>, </span><span><span class=" ">Ziming Xu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Zijun Zhang</span>, </span><span><span class=" ">Kun Zhou</span>, </span><span><span class=" ">Jiaxian Guo</span>, </span><span><span class=" ">Lianhui Qin</span>, </span><span><span class=" ">Biwei Huang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2"></p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A plug-and-play memory module for video diffusion models that enhances physical rule adherence and video fidelity through targeted guidance using low-/high-pass filters.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight"><a href="https://physicsru.github.io/mma.github.io/" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">MMA: Benchmarking multi-modal large language model in ambiguity contexts</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Selena Song</span><sup class="ml-0 text-accent">â€ </sup>, </span><span><span class=" ">Ru Wang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Liang Ding</span>, </span><span><span class=" ">Mingming Gong</span>, </span><span><span class=" ">Yusuke Iwasawa</span>, </span><span><span class=" ">Yutaka Matsuo</span>, </span><span><span class=" ">Jiaxian Guo</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A benchmark evaluating multi-modal large language models&#x27; ability to resolve ambiguities in text using visual context, revealing significant performance gaps.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2502.18273" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Ru Wang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class=" ">Wei Huang</span><sup class="ml-0 text-neutral-600 dark:text-neutral-500">â€ </sup>, </span><span><span class="font-semibold text-accent ">Selena Song</span>, </span><span><span class=" ">Haoyu Zhang</span>, </span><span><span class=" ">Yusuke Iwasawa</span>, </span><span><span class=" ">Yutaka Matsuo</span>, </span><span><span class=" ">Jiaxian Guo</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2"></p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">Investigates Chain-of-Thought reasoning to enhance out-of-distribution generalization in language models, revealing the importance of CoT granularity and sample efficiency.</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 12, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-44d6b2258059bd78.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-44d6b2258059bd78.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-44d6b2258059bd78.js\"],\"default\"]\n7:I[7778,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"252\",\"static/chunks/252-e3d66a9ba9501477.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b9229f4347fd4d10.js\",\"974\",\"static/chunks/app/page-9581e42e39deaab7.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"252\",\"static/chunks/252-e3d66a9ba9501477.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b9229f4347fd4d10.js\",\"974\",\"static/chunks/app/page-9581e42e39deaab7.js\"],\"default\"]\n9:I[5464,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"252\",\"static/chunks/252-e3d66a9ba9501477.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b9229f4347fd4d10.js\",\"974\",\"static/chunks/app/page-9581e42e39deaab7.js\"],\"default\"]\na:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"252\",\"static/chunks/252-e3d66a9ba9501477.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-b9229f4347fd4d10.js\",\"974\",\"static/chunks/app/page-9581e42e39deaab7.js\"],\"default\"]\n11:I[9665,[],\"MetadataBoundary\"]\n13:I[9665,[],\"OutletBoundary\"]\n16:I[4911,[],\"AsyncMetadataOutlet\"]\n18:I[9665,[],\"ViewportBoundary\"]\n1a:I[6614,[],\"\"]\n:HL[\"/_next/static/css/92b53c90e215dddf.css\",\"style\"]\nb:T577,Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coheren"])</script><script>self.__next_f.push([1,"ce, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity.c:T706,@misc{song2025learningplugandplaymemoryguiding,\n  title = {Learning Plug-and-play Memory for Guiding Video Diffusion Models},\n  author = {Selena Song and Ziming Xu and Zijun Zhang and Kun Zhou and Jiaxian Guo and Lianhui Qin and Biwei Huang},\n  year = {2025},\n  eprint = {2511.19229},\n  archivePrefix = {arXiv},\n  primaryClass = {cs.CV},\n  url = {https://arxiv.org/abs/2511.19229},\n  abstract = {Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivate"])</script><script>self.__next_f.push([1,"d by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity.}\n}d:T7da,Multi-Modal Large Language Models (MLLMs) recently demonstrated strong capabilities in both instruction comprehension and responding, positioning them as promising tools for human-computer interaction. However, the inherent ambiguity of language poses a challenge, potentially leading models astray in task implementation due to differing interpretations of the same text within varying contexts. In multi-modal settings, visual information serves as a natural aid in disambiguating such scenarios. In this paper, we introduce the first benchmark specifically designed to evaluate the performance of textbfMLLtextbfMs in textbfAmbiguous contexts (MMA). This benchmark employs a multiple-choice visual question-answering format and includes 261 textual contexts and questions with ambiguous meaning. Each question is linked to a pair of images that suggest divergent scenarios, thus leading to different answers given the same question. These questions are stratified into three categories of ambiguity: lexical,"])</script><script>self.__next_f.push([1," syntactic, and semantic, to facilitate a detailed examination of MLLM performance across varying levels of ambiguity. By evaluating 24 proprietary and open-sourced MLLMs, we find that: (1) MLLMs often overlook scenario-specific information provided by images to clarify the ambiguity of texts. When presented with two different contextual images and asked the same question, MLLMs achieved an accuracy rate of only 53.22% in answering both correctly, compared to human performance at 88.97%.(2) Among the three types of ambiguity, models perform best under lexical ambiguity and worst under syntactic ambiguity. (3) Open-sourced models generally perform significantly lower than proprietary MLLMs, with an average performance gap of 12.59%, Claude 3.5 Sonnet, emerges as the top model, achieving 74.32% accuracy. These findings firstly underscore the current limitations of MLLMs in integrating visual information to clarify textual ambiguities and highlight critical areas for future improvements.e:T95f,"])</script><script>self.__next_f.push([1,"@inproceedings{song2025mma,\n  title = {MMA: Benchmarking multi-modal large language model in ambiguity contexts},\n  author = {Song, Selena and Wang, Ru and Ding, Liang and Gong, Mingming and Iwasawa, Yusuke and Matsuo, Yutaka and Guo, Jiaxian},\n  booktitle = {ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models},\n  year = {2025},\n  abstract = {Multi-Modal Large Language Models (MLLMs) recently demonstrated strong capabilities in both instruction comprehension and responding, positioning them as promising tools for human-computer interaction. However, the inherent ambiguity of language poses a challenge, potentially leading models astray in task implementation due to differing interpretations of the same text within varying contexts. In multi-modal settings, visual information serves as a natural aid in disambiguating such scenarios. In this paper, we introduce the first benchmark specifically designed to evaluate the performance of \\textbf{M}LL\\textbf{M}s in \\textbf{A}mbiguous contexts (MMA). This benchmark employs a multiple-choice visual question-answering format and includes 261 textual contexts and questions with ambiguous meaning. Each question is linked to a pair of images that suggest divergent scenarios, thus leading to different answers given the same question. These questions are stratified into three categories of ambiguity: lexical, syntactic, and semantic, to facilitate a detailed examination of MLLM performance across varying levels of ambiguity. By evaluating 24 proprietary and open-sourced MLLMs, we find that: (1) MLLMs often overlook scenario-specific information provided by images to clarify the ambiguity of texts. When presented with two different contextual images and asked the same question, MLLMs achieved an accuracy rate of only 53.22% in answering both correctly, compared to human performance at 88.97%.(2) Among the three types of ambiguity, models perform best under lexical ambiguity and worst under syntactic ambiguity. (3) Open-sourced models generally perform significantly lower than proprietary MLLMs, with an average performance gap of 12.59%, Claude 3.5 Sonnet, emerges as the top model, achieving 74.32% accuracy. These findings firstly underscore the current limitations of MLLMs in integrating visual information to clarify textual ambiguities and highlight critical areas for future improvements.}\n}"])</script><script>self.__next_f.push([1,"f:T538,Generalization to novel compound tasks under distribution shift is important for deploying transformer-based language models (LMs). This work investigates Chain-of-Thought (CoT) reasoning as a means to enhance OOD generalization. Through controlled experiments across several compound tasks, we reveal three key insights: (1) While QA-trained models achieve near-perfect in-distribution accuracy, their OOD performance degrades catastrophically, even with 10000k+ training examples; (2) the granularity of CoT data strongly correlates with generalization performance; finer-grained CoT data leads to better generalization; (3) CoT exhibits remarkable sample efficiency, matching QA performance with much less (even 80%) data. Theoretically, we demonstrate that compound tasks inherently permit shortcuts in Q-A data that misalign with true reasoning principles, while CoT forces internalization of valid dependency structures, and thus can achieve better generalization. Further, we show that transformer positional embeddings can amplify generalization by emphasizing subtask condition recurrence in long CoT sequences. Our combined theoretical and empirical analysis provides compelling evidence for CoT reasoning as a crucial training paradigm for enabling LM generalization under real-world distributional shifts for compound tasks.10:T6c4,@misc{wang2025indistributionsuccessscalingcurves,\n  title = {Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization},\n  author = {Ru Wang and Wei Huang and Selena Song and Haoyu Zhang and Yusuke Iwasawa and Yutaka Matsuo and Jiaxian Guo},\n  year = {2025},\n  eprint = {2502.18273},\n  archivePrefix = {arXiv},\n  primaryClass = {cs.CL},\n  abstract = {Generalization to novel compound tasks under distribution shift is important for deploying transformer-based language models (LMs). This work investigates Chain-of-Thought (CoT) reasoning as a means to enhance OOD generalization. Through controlled experiments across several compound tasks, we reveal th"])</script><script>self.__next_f.push([1,"ree key insights: (1) While QA-trained models achieve near-perfect in-distribution accuracy, their OOD performance degrades catastrophically, even with 10000k+ training examples; (2) the granularity of CoT data strongly correlates with generalization performance; finer-grained CoT data leads to better generalization; (3) CoT exhibits remarkable sample efficiency, matching QA performance with much less (even 80%) data. Theoretically, we demonstrate that compound tasks inherently permit shortcuts in Q-A data that misalign with true reasoning principles, while CoT forces internalization of valid dependency structures, and thus can achieve better generalization. Further, we show that transformer positional embeddings can amplify generalization by emphasizing subtask condition recurrence in long CoT sequences. Our combined theoretical and empirical analysis provides compelling evidence for CoT reasoning as a crucial training paradigm for enabling LM generalization under real-world distributional shifts for compound tasks.}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"gTfHwqkg1qCnRRbouv_lx\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/92b53c90e215dddf.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Selena Song\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"December 12, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Selena Song\",\"title\":\"\",\"institution\":\"\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"selenasong08@gmail.com\",\"google_scholar\":\"https://scholar.google.com/citations?user=r9zwfxAAAAAJ\u0026hl=en\",\"orcid\":\"https://orcid.org/0009-0009-9433-1647\",\"github\":\"https://github.com/Selena-Song\",\"cv\":\"/papers/CV_SelenaSong.pdf\"},\"features\":{\"enable_likes\":false,\"enable_one_page_mode\":false}}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a Research Assistant at **University of California, San Diego**, working with [Prof. Biwei Huang](https://biweihuang.com/) and [Dr. Kun Zhou](https://lancelot39.github.io/). My research interests lie in **Video Generation**, **Multimodal Reasoning**, and **Causality-Empowered Foundation Models**.\\n\\nPreviously, I received my Master's degree from **The University of Tokyo** in 2025, supervised by [Prof. Yutaka Matsuo](https://scholar.google.com/citations?user=Dy8iau4AAAAJ\u0026hl=en). I earned my Bachelor's degree in Physics from **Fudan University** in 2023.\\n\\nðŸŽ“ **I am actively seeking Ph.D. opportunities for Fall 2026!**\",\"title\":\"About\"}],[\"$\",\"$L9\",\"research_interests\",{\"interests\":[{\"title\":\"Physics-Aware Video Generation\",\"description\":\"Building controllable video generators conditioned on explicit scene states like 3D geometry while enforcing physical constraints and temporal consistency for robust long-horizon video generation.\"},{\"title\":\"Multimodal Reasoning\",\"description\":\"Developing structured representation learning methods to mitigate shortcut reliance under distribution shift. Learning latent state with explicit constraints from multimodal data to support consistent inference and robust generalization.\"},{\"title\":\"Causality-Empowered Foundation Models\",\"description\":\"Developing causal abstractions that support interventions and counterfactual reasoning for multimodal perception and decision-making, leveraging causal graphs and invariant mechanisms for out-of-distribution generalization.\"}],\"title\":\"Research Interests\"}],[\"$\",\"$La\",\"featured_publications\",{\"publications\":[{\"id\":\"song2025learningplugandplaymemoryguiding\",\"title\":\"Learning Plug-and-play Memory for Guiding Video Diffusion Models\",\"authors\":[{\"name\":\"Selena Song\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Ziming Xu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zijun Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kun Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaxian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lianhui Qin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Biwei Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"preprint\",\"status\":\"published\",\"tags\":[\"Computer Vision\",\"Video Generation\",\"Diffusion Models\",\"Memory Networks\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2511.19229\",\"website\":\"https://thrcle421.github.io/DiT-Mem-Web/\",\"abstract\":\"$b\",\"description\":\"A plug-and-play memory module for video diffusion models that enhances physical rule adherence and video fidelity through targeted guidance using low-/high-pass filters.\",\"selected\":true,\"preview\":\"DitMem_00.jpg\",\"bibtex\":\"$c\"},{\"id\":\"song2025mma\",\"title\":\"MMA: Benchmarking multi-modal large language model in ambiguity contexts\",\"authors\":[{\"name\":\"Selena Song\",\"isHighlighted\":true,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Ru Wang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Liang Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Mingming Gong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yusuke Iwasawa\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yutaka Matsuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaxian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Multi-Modal Learning\",\"Large Language Models\",\"Ambiguity Resolution\",\"Benchmarking\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ICLR 2025 Workshop on Navigating and Addressing Data Problems for Foundation Models\",\"website\":\"https://physicsru.github.io/mma.github.io/\",\"abstract\":\"$d\",\"description\":\"A benchmark evaluating multi-modal large language models' ability to resolve ambiguities in text using visual context, revealing significant performance gaps.\",\"selected\":true,\"preview\":\"amb_exp_00.jpg\",\"bibtex\":\"$e\"},{\"id\":\"wang2025indistributionsuccessscalingcurves\",\"title\":\"Beyond In-Distribution Success: Scaling Curves of CoT Granularity for Language Model Generalization\",\"authors\":[{\"name\":\"Ru Wang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Wei Huang\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Selena Song\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yusuke Iwasawa\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yutaka Matsuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaxian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"preprint\",\"status\":\"published\",\"tags\":[\"Language Models\",\"Generalization\",\"Chain-of-Thought\",\"Distribution Shift\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"\",\"website\":\"https://arxiv.org/abs/2502.18273\",\"abstract\":\"$f\",\"description\":\"Investigates Chain-of-Thought reasoning to enhance out-of-distribution generalization in language models, revealing the importance of CoT granularity and sample efficiency.\",\"selected\":true,\"preview\":\"COT_00.jpg\",\"bibtex\":\"$10\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}]],false,false,false]}]]}]]}]}],[\"$\",\"$L11\",null,{\"children\":\"$L12\"}],null,[\"$\",\"$L13\",null,{\"children\":[\"$L14\",\"$L15\",[\"$\",\"$L16\",null,{\"promise\":\"$@17\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"RbWuNr8Jl00-iN6gT3QTa\",{\"children\":[[\"$\",\"$L18\",null,{\"children\":\"$L19\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$1a\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1b:\"$Sreact.suspense\"\n1c:I[4911,[],\"AsyncMetadata\"]\n12:[\"$\",\"$1b\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1c\",null,{\"promise\":\"$@1d\"}]}]\n"])</script><script>self.__next_f.push([1,"15:null\n"])</script><script>self.__next_f.push([1,"19:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n14:null\n"])</script><script>self.__next_f.push([1,"1d:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Selena Song\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Research Assistant at UC San Diego.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Selena Song\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Selena Song,PhD,Research,\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Selena Song\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Selena Song\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Selena Song\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Research Assistant at UC San Diego.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Selena Song's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Selena Song\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Research Assistant at UC San Diego.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n17:{\"metadata\":\"$1d:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>